{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résoud *L-Antique Maze* et apprend les bases du reinforcement learning !\n",
    "```\n",
    "     \n",
    "                                           ..,,;;;;;;,,,,\n",
    "                                     .,;'';;,..,;;;,,,,,.''';;,..\n",
    "                                  ,,''                    '';;;;,;''\n",
    "                                 ;'    ,;@@;'  ,@@;, @@, ';;;@@;,;';.\n",
    "                                ''  ,;@@@@@'  ;@@@@; ''    ;;@@@@@;;;;\n",
    "                                   ;;@@@@@;    '''     .,,;;;@@@@@@@;;;\n",
    "                                  ;;@@@@@@;           , ';;;@@@@@@@@;;;.\n",
    "                                   '';@@@@@,.  ,   .   ',;;;@@@@@@;;;;;;\n",
    "                                      .   '';;;;;;;;;,;;;;@@@@@;;' ,.:;'\n",
    "                                        ''..,,     ''''    '  .,;'\n",
    "                                             ''''''::''''''''\n",
    "                                                                 ,;\n",
    "                                {L-Antique}                     .;;\n",
    "                                                               ,;;;\n",
    "                                --Far From Earth--           ,;;;;:\n",
    "                                                          ,;@@   .;\n",
    "                                                         ;;@@'  ,;\n",
    "                                                         ';;, ,;'        [04/02/2021]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "\n",
    "Bonjour l'équipe technique et bienvenue dans le monde du Reinforcement Learning ! Ici vous allez apprendre diverses techniques pour créer des IA optimisées pour l'apprentissage à partir d'un environement.\n",
    "\n",
    "C'est nottament à partir de ces techniques que les voitures autonomes et les IA des meilleurs jeux vidéos ont été créées.\n",
    "\n",
    "Cette mission est découpée en deux parties :\n",
    "\n",
    "\n",
    "* Tips et partie théorique rédigée par l'équipe scientifique.\n",
    "\n",
    "* Développement d'une IA pouvant se déplacer dans un labyrinthe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pour la suite j'utiliserais quelques abréviations :\n",
    "\n",
    ">RL\n",
    ": *Reinforcement Learning*\n",
    ">DL\n",
    ": *Deep Learning*\n",
    ">MDP\n",
    ": *Markov Decision Process*\n",
    "\n",
    "\n",
    "\n",
    "Le reinforcement learning est un sous-ensemble du machine learning. Dans le domaine de l’apprentissage automatisé, on oppose fréquemment l'apprentissage supervisé et l'apprentissage non supervisé. Le RL fait partie du troisième camp, qui se situe entre le supervisé et non-supervisé. D'un côté, il utilise des éléments du supervisé tel que les réseaux de neurones du Deep Learning pour apprendre à partir de données. Et d'un autre côté, il n'utilise aucune donnée labellisée. L'apprentissage à partir des données se fait donc d'une manière différente.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![baby_img](./img/supervised.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Si je devais vous donner une explication abstraite du RL, je vous dirai ceci :\n",
    "\n",
    "> Le reinforcement learning se compose d'un **agent**, d'un **environnement** et de **récompenses**. Prenons un exemple simple. Imaginons un bébé dans une maison avec ses parents. Le bébé représenterait l'agent et la maison et tout ce qui est autour, tels que les parents, représenterait l'environnement. Le bébé possède un **cerveau vierge**. Il ne connaît rien, mais possède plusieurs sens (la vue, l'ouïe, le touché, ...) qui lui permettent **d'observer** son environnement. Il est important de noter que l'agent ne peut avoir qu'une observation de son environnement et pas un aperçu total à chaque fois. Ce bébé va prendre des **actions** aléatoires et attendre les récompenses venant de l'environnement. S'il touche du feu, il aura **mal** ce qui équivaut à une récompense négative. S'il se met debout et que ses parent **applaudissent**, ce sera une récompense positive.\n",
    "\n",
    "![baby_img](./img/baby_r.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fondement théorique du Reinforcement Learning\n",
    "\n",
    "Vous avez eu un aperçu de ce qu'est le Reinforcement Learning. Maintenant nous allons voir les fondations théoriques qui se cachent derrière et qui vont vous permettre de comprendre en détails cette technique. Dans cette section, nous allons voir ce qu'est un processus de décision markovien, un outil fondamental du RL.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes (MDP)\n",
    "\n",
    "Avant de couvrir le processus de décision markovien complet, nous allons commencer par le cas le plus simple qui est le **Processus de Markov** basique. Ensuite nous étendrons ce processus en rajoutant des récompenses pour arriver au **processus de récompenses markovien**. Enfin, nous finirons par rajouter le dernier élément : les actions. Ceci va nous amener au **Processus de décision markovien**. Ces fondations théoriques sont utilisées dans de nombreux problèmes d'informatique et vous serviront en dehors du Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Markov Process\n",
    "\n",
    "Le processus de Markov (aussi appelé Chaîne de Markov) est un processus dans lequel vous ne pouvez qu'observer. Imaginez que vous ayez un système devant vous et que vous l'observiez sans avoir aucune influence sur celui-ci. Ce que vous observez est appelé **l'état du système**, et ce système peut passer d'un état à un autre en fonction de certaines règles. Encore une fois, je répète que vous n'êtes qu'un observateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple est toujours plus parlant qu'une explication. Imaginez un robot se déplaçant dans un labyrinthe de 9 cases. Vous êtes un observateur qui examine le déplacement du robot à chaque instant *t*. Le robot et le labyrinthe représentent le système, et l'état du système représente la position du robot dans le labyrinthe à un instant *t*. Le robot bouge à chaque instant de case en case (et donc d'état en état) en suivant des règles, mais vous n'avez aucune influence sur le déplacement d'un état *t* à un état *t+1*.\n",
    "\n",
    "L'ensemble des états possibles d'un système est appelé le **State Space**. Dans l'exemple que nous avons pris le *State Space* a une taille de 9, car il y a 9 cases dans le labyrinthe. Ce *State Space* doit avoir une taille finie, mais il peut être extrêmement grand. Ci-dessous, on a représenté les changements d'état du robot au cours du temps. Une séquence d'observations au cours du temps est appelée *Chain Of State*, et forme ce qu'on appelle un **historique**.\n",
    "\n",
    "---\n",
    "![maze_img](./img/maze.gif)![transition_img](./img/transition.png)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous l'avez vu, on représente souvent un système et ses changements d'états (globalement les MDP) par un graphe, dans lequel chaque nœud est un état, et chaque arrête est une transition entre deux états.\n",
    "\n",
    "La probabilité de passer d'un état à un autre est définie dans le système, et l'observateur n'a pas accès à ces probabilités. Il ne peut qu'observer le système et essayer d'estimer ces probabilités.\n",
    "\n",
    "Plus on possède d'observations, plus notre approximation sera proche des probabilités réelles de changement d'état du système. Dans notre exemple, nous allons définir les probabilités du système. Ainsi nous aurons accès aux probabilités réelles, mais il est important de les distinguer des probabilités estimées. Les probabilités réelles sont fixes et ne changent pas, alors que les probabilités estimées évoluent au fur et à mesure de nos nouvelles observations. N'oublions pas que le système est immuable : nous nous ne faisons que l'observer.\n",
    "\n",
    "Un autre élément important est la **Propriété de Markov**. Pour qu'un système soit appelé un processus de Markov, il faut que les états du système soient distinguables les uns des autres et uniques. Cela nous permet de n'avoir à connaitre qu'un seul et unique état pour prédire l'état suivant du système, et donc de ne pas avoir à connaître tout l'historique pour effectuer une prédiction sur le système.\n",
    "\n",
    "> En probabilités, un processus *stochastique* vérifie la propriété de Markov si et seulement si la distribution conditionnelle de probabilité des états futurs, étant donné les états passés et l'état présent, ne dépend que de l'état présent et non pas des états passés, soit un processus ayant une absence de « mémoire ». Un processus qui possède cette propriété est appelé processus de Markov. Dans un tel processus, la meilleure prévision que l'on puisse faire du futur, en connaissant le passé et le présent, est identique à la meilleure prévision qu'on puisse faire du futur en connaissant uniquement le présent. Si on connait le présent, la connaissance du passé n'apporte pas d'informations supplémentaires utiles pour la prédiction du futur.\n",
    "[*Propriété de Markov Wikipedia*](https://fr.wikipedia.org/wiki/Propri%C3%A9t%C3%A9_de_Markov)\n",
    "\n",
    "![proba](./img/proba.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Reward Process\n",
    "\n",
    "> Une **transition** signifie le passage d'un état à un autre\n",
    "> Un **épisode** est une suite de transition (exemple: wake up => code => deploy => sleep)\n",
    "\n",
    "Maintenant que nous avons la base, les processus de Markov, introduisons le deuxième élément : **les récompenses**. Nous allons associer une récompense (Reward) à chacune de nos **transitions**. Les récompenses sont des nombres qui peuvent être positifs ou négatifs.\n",
    "\n",
    "Maintenant que nous avons des récompenses, notre objectif est de maximiser le nombre de récompenses acquises lors des **épisodes**.\n",
    "\n",
    "Pour chaque épisode, on mesure le rendement *G*, qui est la somme totale des récompenses. L'objectif est de **maximiser** ce rendement.\n",
    "![return](./img/return.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Un autre élément important est ce qu'on appelle le **discount factor**, généralement noté γ (gamma). C'est un nombre compris entre 0 et 1. C'est un peu compliqué, mais restez attentif et ouvert d'esprit : ) c'est important. N'hésitez pas à faire appel aux encadrants.\n",
    "\n",
    "Essayons de comprendre le calcul ci-dessus.\n",
    "Pour chaque transition de notre **State Space**, nous récoltons récompense dans l'état *t + 1* d'arrivée. Prenons un exemple avec un nouveau système plus proche de la vie réelle.\n",
    "\n",
    "![reward](./img/reward.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque élément de notre épisode, nous calculons le **rendement** comme étant la somme des récompenses futures.\n",
    "\n",
    "Prenons un exemple avec cette épisode (wake up => code => deploy => sleep) : étant donné notre épisode commençant par l'état wake up, quelle est le rendement estimé en étant dans l'état wake up ?\n",
    "\n",
    "Autrement dit, nous allons faire la somme des récompenses de tous les états de cet épisode, soit : -3 + 10 + 3 = 10.\n",
    "\n",
    "On vient donc de calculer le rendement de wake up pour cet épisode.\n",
    "\n",
    "Cependant, il manque un élément. Les récompenses arrivant dans un futur lointain sont moins importantes que celles arrivant dans un futur proche. Les récompenses qui arrivent dans le futur vont ainsi être multipliées par le **discount factor** (gamma) élevé à la puissance *t*, *t* étant le temps avant que la récompense soit obtenue. On obtiendra des résultats différents suivant la valeur de gamma :\n",
    "\n",
    "* Si gamma est égal à 1, le rendement sera égal à la somme de toutes les récompenses futures de l'épisode, ce qui correspond à une visibilité parfaite de toutes les récompenses futures de l'épisode.\n",
    "\n",
    "* Si gamma est égal à 0, alors le rendement sera égal à la récompense immédiate sans aucune récompense future, et cela correspond donc à une visibilité inexistante sur le futur, soit une sorte de myopie.\n",
    "\n",
    "Le gamma est un paramètre très important en RL. Pensez-y comme un paramètre déterminant jusqu'à quelle distance dans le futur votre modèle regarde pour estimer le rendement. Plus gamma est proche de 1 plus votre vision est lointaine et plus gamma est proche de 0 plus vous êtes myope : ).\n",
    "\n",
    "![return](./img/return.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Bon, j'espère ne pas vous avoir perdu, vous y êtes presque !\n",
    "\n",
    "* Actuellement le calcul de ce rendement G n'est pas vraiment utile, car il n'est défini que pour tous les épisodes spécifiques que nous observons. Ce rendement G est donc calculé pour chaque épisode spécifique, et peut varier d'un épisode a l'autre même pour un état identique.\n",
    "\n",
    "* Cependant si l'on calcule **l'espérance du rendement** de chaque état du système, c'est-à-dire en faisant la moyenne du rendement d'énormément d'épisodes, on obtient une valeur très utile appelée **value state** qui nous indique à quel point il est utile d'être dans un état en particulier.\n",
    "\n",
    "Pour chaque état du système la valeur *v(s)* est la moyenne des rendements G de cet état *s* sur des épisodes différents.\n",
    "\n",
    "![value](./img/valuefunction.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.  Markov Decision Process\n",
    "\n",
    "> Ici une **transition** correspond à ceci : (état => action => nouvelle état => récompense)\n",
    "\n",
    "Nous arrivons maintenant à notre dernier élément. Je pense que vous avez déjà une petite idée de comment étendre le processus de Markov pour obtenir un processus de décision markovien.\n",
    "\n",
    "* Premièrement nous allons ajouter un ensemble d'actions qui doit avoir une taille finie. C'est ce qu'on appelle **l'Action Space**.\n",
    "\n",
    "* Deuxièmement nous allons relier les actions avec les changements d'état et les récompenses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![markov](./img/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "Dans le cas d'un processus de Markov on utilise une matrice de transition, un tableau à 2 dimensions qui contient les **probabilités réelles** de passer d'un état 1 à un état 2 dans le système. Petit exemple avec le schéma ci-dessous. La probabilité d'être dans l'état *s1* et de passer à l'état *s2 est *p12*.\n",
    "\n",
    "![process](./img/matrice.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons rajouté les actions, il faut que l'on transforme notre matrice de transitions pour les prendre en compte. Nous allons donc passer d'une matrice à 2 dimensions à une matrice à 3 dimensions.\n",
    "\n",
    "Nous n'observons plus passivement le système : nous pouvons activement choisir de prendre une action à chaque changement d'état.\n",
    "\n",
    "Nous avons une matrice dont la profondeur représente les actions que l'agent peut prendre et dont les deux autres dimensions représentent l'état source et l'état d'arrivée. Ci-dessous un schéma récapitulatif.\n",
    "\n",
    "![mdp_pro](./img/mdp_rpo.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les actions que l'on prend peuvent maintenant influencer la probabilité d'arriver dans un certain état de destination.\n",
    "\n",
    "Cependant on pourrait se demander pourquoi les actions ne font pas tomber dans l'état de destination avec une probabilité de 100 %.\n",
    "\n",
    "Reprenons l'exemple avec le robot de tout à l'heure. Ce robot peut avoir des moteurs défectueux, et lorsque qu'il prend une action il y a une probabilité non nulle qu'un de ses moteurs vrille et qu'il tombe dans la case à côté de celle dans laquelle il souhaitait aller. Si nous demandons au robot d'aller en haut, il y a par exemple 90% de chance qu'il arrive sur la case d'en haut, ainsi que 10% de chance qu'il ne bouge pas ou qu'il tombe sur une autre case. Comme dans la réalité, les actions que l'on fait n'aboutissent pas forcément au résultat qu'on espérait. Il peut toujours y avoir des imprévus.\n",
    "\n",
    "La récompense que l'on obtient dépend maintenant de l'action qui emmène vers l'état cible et plus seulement de l'état source.\n",
    "\n",
    "C'est similaire à la vie réelle. Lorsque l'on met des efforts dans quelque chose, on gagne généralement de l'expérience même si les résultats de nos efforts ne sont pas forcément formidables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bravo** d'avoir tenu jusqu'ici. On a couvert le plus important pour le RL et le MDP. La dernière chose dont nous devons parler rapidement est de la **Policy**. Ensuite, vous aurez un vocabulaire complet et vous ne passerez plus pour des intrus lors de discussions sur le RL : ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Kaneky !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Après la théorie la pratique !** Nous avons introduit de nombreux concepts théoriques, mais cela va nous permettre de ne pas le refaire sur les prochaines missions.\n",
    "\n",
    "Ici, nous allons essayer d'utiliser la **Value Function** pour résoudre un labyrinthe. Un agent va se balader dans un labyrinthe dans lequel se trouve une Target qu'il devra aller chercher. La Target rapporte une récompense de 10 et le feu un malus de 5. L'agent va se balader des milliers de fois dans le labyrinthe et apprendre petit à petit à trouver la Target. Le labyrinthe sera affiché sur le terminal, ainsi que les valeurs de tous les états *V(s)*, qui s'ajusteront en temps réel. Vous comprendrez que la **Value Function** représente le **cerveau** de l'agent.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par importer certains Tools qui nous seront utiles ainsi que l'environnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "from Envi import Env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**PS: Respectez bien le type de retour de chaque fonction. Une liste contenant un seul int n'est pas la même chose qu'un int !**\n",
    "\n",
    "la première tâche à effectuer est de définir certaines constantes ainsi qu'un **Named Tuple** appelé `Transition` représentant les transitions (`state`, `action`, `next_state`, `reward`). On utilisera ce Named Tuple comme un type semblable à une liste de taille 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = # Define namedtuple\n",
    "BATCH_SIZE = # Define batch size\n",
    "GAMMA = # Define gamma\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un élément fondamental en RL est que l'agent doit posséder une **mémoire**. Cette mémoire va **stocker plusieurs transitions passées**. Comme cela, même si l'agent vient d'effectuer une nouvelle transition, il aura en mémoire pendant encore un moment la transition précédent, et continuera de mettre à jour la **Value Function** de cette transition. Cela permet à l'agent de s'entraîner plusieurs fois sur des transitions qu'il ne verrait que **très peu de fois**.\n",
    "\n",
    "Nous allons donc créer une classe `Memory`.\n",
    "\n",
    "Initialisez cette classe avec :\n",
    "\n",
    "* Une variable `capacity` contenant le nombre de transitions que la mémoire peut stocker. Quand elle est pleine, la mémoire va oublier les transitions les plus anciennes.\n",
    "\n",
    "* Une variable `memory` contenant la liste des transitions (la mémoire en elle-même).\n",
    "\n",
    "* Une variable `position` contenant la position dans la mémoire de la dernière transition ajoutée. Cela va nous permettre de rajouter des transitions au bon endroit et de retirer les plus anciennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        # Need some code\n",
    "    \n",
    "    def clear(self):\n",
    "        self.memory: list = []\n",
    "        self.position: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_memory : Memory = Memory(1000)\n",
    "\n",
    "assert (test_memory.capacity == 1000)\n",
    "assert (test_memory.memory == [])\n",
    "assert (test_memory.position == 0)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Nous allons créer une fonction `push()` dans `Memory`. Cette fonction va prendre en paramètre les paramètres nécessaires pour créer notre transition (param1 = stata, param 2 = next_state, etc ...). Nous commençons par vérifier si la taille de la mémoire est inférieure à la capacité. Si c'est le cas, nous ajoutons un élément `None` à la mémoire pour créer une nouvelle case vide.\n",
    "\n",
    "* Ensuite nous nous plaçons dans la case actuelle mémoire grâce à notre variable `position` et nous ajoutons à cet endroit une nouvelle transition, créée à partir des paramètres de la fonction.\n",
    "\n",
    "* Nous pouvons maintenant mettre à jour la variable `position` en suivant la règle suivante : La position augmente de 1, mais si elle devient supérieure à la capacité, la position revient à 0. Pour cela, vous pouvez utiliser un modulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(self, *args: list):\n",
    "    \"\"\"push a transition\"\"\"\n",
    "    # Need some code\n",
    "\n",
    "setattr(Memory, 'push', push)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "dumy1_state = [28]\n",
    "dumy1_action = [3]\n",
    "dumy1_next_state = [44]\n",
    "dumy1_reward = [-5]\n",
    "\n",
    "\n",
    "test_memory : Memory = Memory(2)\n",
    "test_memory.clear()\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward)\n",
    "assert (test_memory.position == 0)\n",
    "assert (test_memory.memory == [Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "\n",
    "test_memory.push(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward)\n",
    "assert (test_memory.position == 1)\n",
    "assert (test_memory.memory == [Transition(dumy1_state, dumy1_action, dumy1_next_state, dumy1_reward), Transition(dumy_state, dumy_action, dumy_next_state, dumy_reward)])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nous allons ensuite implémenter deux autres fonctions qui nous seront utiles. La fonction `batch()` qui va nous renvoyer un **Sample** (échantillon) de *n* transitions, prélevées au hasard dans la mémoire de l'agent. Cela va nous permettre de mettre à jour la valeur des états avec de nouvelles données. Vous pouvez utiliser `random.sample()` pour implémenter la fonction `batch()`. Implémentez aussi la fonction `len()` qui renvoit la taille actuel de la mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(self, batch_size):\n",
    "    return # Need some code\n",
    "\n",
    "def __len__(self):\n",
    "    return # Need some code\n",
    "\n",
    "setattr(Memory, '__len__', __len__)\n",
    "setattr(Memory, 'batch', batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumy_state = [43]\n",
    "dumy_action = [3]\n",
    "dumy_next_state = [44]\n",
    "dumy_reward = [10]\n",
    "\n",
    "test_memory.capacity = 20\n",
    "test_memory.clear()\n",
    "[test_memory.push(dumy_state, dumy_action, dumy_next_state, dumy_reward) for i in range(20)]\n",
    "\n",
    "dumy_batch = test_memory.batch(10)\n",
    "\n",
    "assert (len(dumy_batch) == 10)\n",
    "for i in range(10) :\n",
    "     assert(dumy_batch[i].state == [43] and dumy_batch[i].action == [3] and dumy_batch[i].next_state == [44] and dumy_batch[i].reward == [10])\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Maintenant, nous allons nous attaquer à la classe principale: La classe `Agent`.\n",
    "\n",
    "* Cette classe prend en paramètre de son constructeur **l'environnement**, qui est une instance de `Env`.\n",
    "\n",
    "* Créez une variable `memory` qui contient une instance de la classe `Memory`. Cette mémoire doit avoir une capacité de 2000 pour le moment.\n",
    "\n",
    "* Créez une variable `env` pour stocker l'environnement reçu.\n",
    "\n",
    "* Créez une variable `V` qui contient une liste ne contenant que des zéros, et qui fait la même taille que le nombres d'états dans l'environnement. Pour cela utilisez la fonction `get_map()` de l'environnement.\n",
    "\n",
    "* Ensuite appelez la fonction `self.init_v()` qui va initialiser les valeurs des états où se trouvent les **Targets** et les **feux** avec leur valeur par défaut. Ces valeurs seront égales à la récompense que donnent ces cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: Env):\n",
    "        # Need some code\n",
    "        self.init_v()\n",
    "\n",
    "    def init_v(self):\n",
    "        map = env.get_map()\n",
    "        for j in range(len(map)):\n",
    "            if map[j] == 'T':\n",
    "                self.V[j] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "test_agent : Agent = Agent(env)\n",
    "\n",
    "assert (test_agent.memory.capacity == 2000)\n",
    "assert (len(test_agent.V) == len(env.get_map()))\n",
    "assert (test_agent.env != None)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous avez déjà accès à quelques fonctions utilitaires pour **Save** et **Load** la valeur des états."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_v(self, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        for v_f in self.V:\n",
    "            f.write(str(round(v_f, 4)) + \" \")\n",
    "\n",
    "def load_v(self, path: str):\n",
    "    with open(path, 'r') as f:\n",
    "        self.V = f.read().split(\" \")\n",
    "        del self.V[len(self.V) - 1]\n",
    "        for k in range(len(self.V)):\n",
    "            self.V[k] = float(self.V[k])\n",
    "\n",
    "setattr(Agent, 'save_v', save_v) \n",
    "setattr(Agent, 'load_v', load_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la vie réelle, quand nous avons pris l'habitude de faire quelque chose, nous le refaisons encore et encore sans y réfléchir, mais quelques fois nous divergeons de nos habitude. Prenons un exemple : je suis un élève qui sort de l'école et pour rentrer chez moi, je prends toujours le même chemin, car c'est celui-ci que l'on m'a appris et que je prends tous les jours.\n",
    "\n",
    "Un beau jour, je me trompe par hasard de chemin et je découvre un chemin plus rapide pour rentrer chez moi, et depuis ce jout j'utilise ce nouveau chemin.\n",
    "\n",
    "Et voilà ! Vous venez de voir à travers cette comparaison le concept d'exploration et d'exploitation. Même si notre agent a appris quelque chose, il faut toujours qu'il y ait une chance qu'il prenne des actions différentes pour peut-être découvrir de nouvelles choses et s'améliorer. Le pourcentage de chance que l'agent prenne une action aléatoires plutôt que la meilleur action qu'il connaisse est ce qu'on appelle **l'epsilon**. Au début *epsilon* est très haut, car l'agent doit apprendre, et donc explorer de nombreuses possibilités. Mais au fur et à mesure *epsilon* diminue, ce qui pousse l'agent à choisir des actions en fonction de ce qu'il a appris, c'est-à-dire en utilisant la value fonction.\n",
    "\n",
    "Nous allons maintenant implémenter la fonction `greedy_step()`. C'est la fonction qui va prendre la meilleure décision possible en fonction de ce que l'agent a appris.\n",
    "\n",
    "* Commençons par créer un tableau d'actions. Il y a quatre actions possibles : haut, bas, gauche, droite (1,2,3,4). On stocke ce tableau dans une variable nommée `actions`.\n",
    "\n",
    "* Maintenant nous allons réfléchir. L'agent est à une position *t*. Nous souhaitons savoir quelle action l'agent doit prendre en fonction de ce qu'il a appris. Nous savons que la value nous permet de savoir à quel point chaque état nous permet de maximiser notre rendement. L'environnement contient une fonction `predict(action)` qui renvoie une liste contenant un seul élément : la case sur laquelle l'agent se retrouverait s'il prenait cette action.\n",
    "\n",
    "* Nous savons également que chaque état possède une **value**. Nous allons donc itérer sur toutes les actions et regarder sur quel état cette action emmène l'agent, puis nous allons regarder la valeur de chacun de ces état grâce à **self.V[s]**. Nous allons enfin renvoyer l'action qui ammène à l'état ayant la lus grande value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_step(self) -> int:\n",
    "    actions = [1, 2, 3, 4]\n",
    "    # Need some Code\n",
    "    # Tips: Predict takes an action and return [next_state of agent] \n",
    "    # Tips : self.V is an array with value for all state so self.V[state] return the value function for that state\n",
    "    return # Return Action (an int between 1 and 4)\n",
    "\n",
    "setattr(Agent, 'greedy_step', greedy_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.greedy_step() == 2)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nous allons maintenant implémenter la fonction `take_action()` qui va effectuer les **actions**. Elle reçoit en paramètre l'état actuel de l'agent et epsilon.\n",
    "\n",
    "* Nous allons prendre un nombre entre 0 et 1, si ce nombre est inférieur à **epsilon** l'agent prend une action au hasard, et nous renvoyons un nombre aléatoire parmu les actions possibles.\n",
    "\n",
    "* Si jamais le nombre est supérieur à **epsilon**, nous allons appeler notre fonction `greedy_step()`, mettre sa valeur de retour dans une liste, et renvoyer ce tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(self, eps_t: int) -> [int]:\n",
    "    # Need some code (if .... return/ else .... return WARNING: cf return type)\n",
    "    # Tips second line : Need to use greedy_step function that return an int between 1 and 4 and wrap that int into an array ([int])\n",
    "    # Tips first line : random.randint/random.uniform \n",
    "\n",
    "setattr(Agent, 'take_action', take_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env: Env = Env(\"maze.txt\")\n",
    "\n",
    "test_agent.load_v(\"test.txt\")\n",
    "assert(test_agent.take_action(0) == [2])\n",
    "assert(test_agent.take_action(1)[0] >= 1 and test_agent.take_action(1)[0] <= 4)\n",
    "print(\"\\n\\033[92mVery good ! go to the next step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rappel pour la suite :\n",
    "Afin d'acquérir des récompenses, l'agent doit maximiser son rendement. La fonction de valeur est le moyen efficace de déterminer la valeur d'un état, soit à quel point il contribue à augmenter le rendement. On note cette fonction *V(s)*. Elle mesure les futures récompenses que l'agent pourrait potentiellement obtenir en étant dans cet état *s*.\n",
    "![rappel fonction de valeur](./img/rapp.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Voici un rappel de comment calculer le rendement d'un état d'un épisode :\n",
    "![calcul rendement](./img/return.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Souvenez vous également que la valeur d'un état est égale à l'espérance de tous les rendements de cet état sur tous les épisodes.\n",
    "![calcul value](./img/valuefunction.png)\n",
    "\n",
    "\n",
    "\n",
    "> Nous pouvons maintenant modifier cette expression. On mets à jour la valeur de chaque état grâce à cette formule qui reprend les deux précédentes. On prend la valeur de l'état actuel et on lui rajoute sa récompense, ainsi que *gamma* multiplié par la différence entre la valeur de l'état *t + 1* et celle de l'état actuel.\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V(s) = V(s) + r + \\gamma * [V(s') - V(s)]\">\n",
    "\n",
    "> Nous répétons cette opération autant de fois que nous avons de transition, afin d'ajuster les valeurs de tous les états visités, et ainsi approcher l'espérance réelle de cet état.\n",
    "\n",
    "\n",
    "> Cependant, comme lorsqu'on entraîne les paramètres d'un réseau de neurones, on doit rajouter un **Learning rate** (taux d'apprentissage) qui va permettre d'ajuster petit à petit la valeur de l'état plutôt que de faire de gros ajustements qui rendront notre valeur instable.\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=V(s) = V(s) + \\alpha * [r + \\gamma * [V(s') - V(s)]]\">\n",
    "\n",
    "\n",
    "> *r* = reward\n",
    "> *s* = state\n",
    "> *s'* = next_state\n",
    "> *gamma* = gamma\n",
    "> *alpha* = Learning rate\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Maintenant, implémentez la fonction `learn()` de l'agent. Cette fonction est responsable de la mise des **Values** de chaque état.\n",
    "\n",
    "* Récupérez des transitions depuis la mémoire de l'agent (Récupérez BATCH_SIZE transitions), grâce à la fonction `batch()` que vous avez implémenté tout à l'heure. Stockez ce que vous avez récupéré dans une variable `transitions`.\n",
    "\n",
    "* Il va falloir faire quelques **prints** et tester ce que vous avez récupéré de la fonction batch. Pour l'instant, vous avez récupéré une liste de Named tuples `Transition`, mais nous ne voulons qu'un seul named tuple `Transition` contenant des listes. Pour cela, il vous faudra utiliser l'opérateur `*` et la fonction `zip()`.\n",
    "\n",
    "* Ensuite il vous faudra réutiliser `*` sur la valeur de retour de `zip()` pour transformer à nouveau les valeurs en un named tuple `Transition`. Stockez cette `Transition` dans une variable `batch`.\n",
    "\n",
    "* N'oubliez pas d'afficher vos données pour voir les données que vous avez obtenu. Normalement, vous pouvez accéder à tous les états avec `batch.state` puis `batch.reward` pour les récompenses, etc. Votre variable contient donc quatre listes.\n",
    "\n",
    "* Il vous faut maintenant itérer sur `batch.state`, `batch.next_state` et `batch.reward`. Vous pouvez faire une boucle for avec un `zip()` et récupérer un élément de chacun des tableaux à chaque tour de boucle.\n",
    "\n",
    "* Vous l'avez remarqué, chaque state est un tableau à 1 élément. Initialisez donc trois variable `s`, `s_prime` et `reward` avec le premier élément des listes respectives (`state`, `next_state` et `reward`).\n",
    "\n",
    "* Une fois cela fait, calculez la nouvelle valeur de `self.V[s]` avec la formule que nous avons vu ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self):\n",
    "    transitions = self.memory.batch(BATCH_SIZE)\n",
    "    #On passe d'un tableau de transition a une transition de tableau\n",
    "    #On met * pour passer l'arg à la fonction zip puis * pour unzip sous forme de list\n",
    "    batch: Transition = Transition(*zip(*transitions))\n",
    "    for state, next_state, reward in zip(batch.state, batch.next_state, batch.reward):\n",
    "        # Need one line of code (look above there is a formula with v[s] = ....)\n",
    "\n",
    "setattr(Agent, 'learn', learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isTired = False\n",
    "assert (isTired)\n",
    "print(\"\\n\\033[92mVery good ! go to last step ...\\033[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Et voici la dernière fonction : la fonction `main`.\n",
    "\n",
    "* Nous commençons par créer une instance de l'environnement en lui passant le fichier txt contenant le labyrinthe.\n",
    "\n",
    "* Nous créons une instance de l'agent avec l'environnement.\n",
    "\n",
    "* Nous définissons deux variables, une pour compter le nombre d'actions effectuée, et une contenant **epsilon**.\n",
    "\n",
    "* Ensuite nous effectuons le nombre d'action que nous voulons.\n",
    "\n",
    "* Toute les 10 itérations nous diminuons **epsilon**.\n",
    "\n",
    "* Nous récupèrons **l'état actuel de l'agent**. (la fonction `get_env()` de l'environnement renvoie l'état actuel de l'agent)\n",
    "\n",
    "* Nous demandons à l'agent quelle action il souhaite prendre.\n",
    "\n",
    "* Nous passons cette action à l'environnement grâce à la fonction `step()` qui va effectuer l'action.\n",
    "\n",
    "* Nous ajoutons la transition que l'agent vient de faire dans sa mémoire.\n",
    "\n",
    "* Nous appelons `render()` pour afficher l'état actuel de l'environnement.\n",
    "\n",
    "* Tant que nous n'avons pas aux moins 128 transitions dans la mémoire de l'agent, nous stockons les transitions.\n",
    "\n",
    "* Une fois que l'agent possède 128 transitions dans sa mémoire, il va apprendre et affiner sa **value function** en utilisant ces transitions, et chaque fois que nous rajouterons une transition dans la mémoire l'agent va en supprimer une autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Bienvenue dans L-Antique Game n°1 ! Vous allez apprendre toutes sortes de choses sur le Reinforcement Learning !\")\n",
    "    env: Env = Env(\"maze.txt\")\n",
    "    agent: Agent = Agent(env)\n",
    "    i = 0\n",
    "    eps = 0.9\n",
    "    #agent.load_v(\"save.txt\")\n",
    "    while i < 10000:\n",
    "        if i % 10 == 0:\n",
    "            eps = max(eps * 0.9998, 0.05)\n",
    "        state: list = env.get_env()\n",
    "        action: list = agent.take_action(eps)\n",
    "        next_state, reward = env.step(action[0])\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "        env.render(agent.V, eps, 0.1)\n",
    "        i += 1\n",
    "        if i > 128:\n",
    "            agent.learn()\n",
    "    #agent.save_v(\"save.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}